{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.dourthe.tech'> <img src='Dourthe_Technologies_Headers.png' /></a>\n",
    "___\n",
    "<center><em>For more information, visit <a href='http://www.dourthe.tech'>www.dourthe.tech</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Prediction using Logistic Regression\n",
    "\n",
    "___ \n",
    "## Objective\n",
    "Train a Logistic Regression model to identify fraudulent credit card transactions.\n",
    "\n",
    " ___\n",
    "## Dataset\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "The dataset can be found at: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Libraries Imports\n",
    "### Data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new synthetic points in an unbalanced dataset using the Synthetic Minority Over-sampling Technique (SMOTE)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split data between training and test set\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Data scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a pipeline for imbalanced datasets\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Support vector machine classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Model save\n",
    "import pickle\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('credit_card_fraud.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Exploratory Data Analysis\n",
    "### General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the first few rows of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "**Check for missing data (expressed as a percentage of the dataset length).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().max()*100/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thankfully, there are no missing data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imbalance\n",
    "\n",
    "**Let's compare the number of observations for each class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('Class', data=df, palette='coolwarm')\n",
    "plt.title('Number of Regular (0) and Fraudulent (1) transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the description of the dataset, it is extremely unbalanced.\n",
    "\n",
    "**Let's calculate the ratio between classes to better understand the imbalance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of non-fraudulent transactions:\t 99.83 %\n",
      "Percentage of fraudulent transactions:\t\t 0.17 %\n"
     ]
    }
   ],
   "source": [
    "valuecount = df['Class'].value_counts()*100/len(df)\n",
    "print('Percentage of non-fraudulent transactions:\\t', valuecount[0].round(2), '%')\n",
    "print('Percentage of fraudulent transactions:\\t\\t', valuecount[1].round(2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such a large imbalance, most classifiers would not be able to detect distinguishing patterns between non-fraudulent and fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Machine Learning\n",
    "**To account for the large imbalance of the dataset, we will use cross-validation combined with the Synthetic Minority Over-sampling Technique (SMOTE).**\n",
    "### Input/Output definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "**Instead of using the usual train_test_split function from SciKit.Learn, we will use the StratifiedKFold method.**\n",
    "This cross-validation technique is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare original label distribution to train/test split label distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABELS DISTRIBUTION\n",
      "\tOriginal:\t\t 99.83 % (label 0)\t 0.17 % (label 1)\n",
      "\tTraining set:\t\t 99.83 % (label 0)\t 0.17 % (label 1)\n",
      "\tTest set:\t\t 99.83 % (label 0)\t 0.17 % (label 1)\n"
     ]
    }
   ],
   "source": [
    "_, original_labels_count = np.unique(y, return_counts=True)\n",
    "original_dist = np.round(original_labels_count/len(y)*100, 2)\n",
    "_, train_labels_count = np.unique(y_train, return_counts=True)\n",
    "train_dist = np.round(train_labels_count/len(y_train)*100, 2)\n",
    "_, test_labels_count = np.unique(y_test, return_counts=True)\n",
    "test_dist = np.round(test_labels_count/len(y_test)*100, 2)\n",
    "\n",
    "print('LABELS DISTRIBUTION')\n",
    "print('\\tOriginal:\\t\\t', original_dist[0], '% (label 0)\\t', original_dist[1], '% (label 1)')\n",
    "print('\\tTraining set:\\t\\t', train_dist[0], '% (label 0)\\t', train_dist[1], '% (label 1)')\n",
    "print('\\tTest set:\\t\\t', test_dist[0], '% (label 0)\\t', test_dist[1], '% (label 1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling\n",
    "**To prevent the potential impact of higher weights from variables that have larger ranges of values, we need to scale the data.**\n",
    "\n",
    "We can start by defining a scaler and fitting it to the training data.\n",
    "\n",
    "_Warning: do not fit the scaler to the test data, as in a real life scenario we would not be able to fit the scaler to the testing data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we can transform the training and test data.**\n",
    "\n",
    "Note: no need to transform the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_train, y_test = y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and fit model to training data\n",
    "**First, we need to define a grid of parameters that will be used during cross-validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we will define an instance of RandomizedSearchCV using LogisticRegression as the estimator.**\n",
    "\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter. This will allow the training to be a little more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_logR = RandomizedSearchCV(LogisticRegression(max_iter=300), param_grid, n_iter=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we will create a loop that will iterate through each fold generated by the StratifiedKFold method and create a pipeline that will apply the SMOTE method to the dataset and run our defined classifier.**\n",
    "\n",
    "_Note: a variety of evaluation scores will be saved throughout the process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\bdour\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Implementing SMOTE Technique \n",
    "for train, test in skf.split(X_train_scaled, y_train):\n",
    "    pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), randomized_logR)\n",
    "    model = pipeline.fit(X_train_scaled[train], y_train[train])\n",
    "    best_model = randomized_logR.best_estimator_\n",
    "    predictions = best_model.predict(X_train_scaled[test])    \n",
    "\n",
    "    accuracy_lst.append(pipeline.score(X_train_scaled[test], y_train[test]))\n",
    "    precision_lst.append(precision_score(y_train[test], predictions))\n",
    "    recall_lst.append(recall_score(y_train[test], predictions))\n",
    "    f1_lst.append(f1_score(y_train[test], predictions))\n",
    "    auc_lst.append(roc_auc_score(y_train[test], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'credit_card_fraud_predictor.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "**First, let's print the mean evaluation scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION\n",
      "\tMean accuracy:\t 0.96\n",
      "\tMean precision:\t 0.07\n",
      "\tMean recall:\t 0.91\n",
      "\tMean f1-score:\t 0.13\n"
     ]
    }
   ],
   "source": [
    "print('MODEL EVALUATION')\n",
    "print('\\tMean accuracy:\\t',  np.round(np.mean(accuracy_lst), 2))\n",
    "print('\\tMean precision:\\t', np.round(np.mean(precision_lst), 2))\n",
    "print('\\tMean recall:\\t', np.round(np.mean(recall_lst), 2))\n",
    "print('\\tMean f1-score:\\t', np.round(np.mean(f1_lst), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then, we will generate predictions using the best model generated during cross-validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model using classification report and confusion matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX\n",
      "\n",
      "Total number of\n",
      "\tTrue positives:\t\t 84\n",
      "\tTrue negatives:\t\t 56325\n",
      "\tFalse positives:\t 538 \t\tType I error\n",
      "\tFalse negatives:\t 14 \t\tType II error\n",
      "\n",
      "\n",
      "Correct classifications:\t 99.03 %\n",
      "Incorrect classifications:\t 0.97 %\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56863\n",
      "           1       0.14      0.86      0.23        98\n",
      "\n",
      "    accuracy                           0.99     56961\n",
      "   macro avg       0.57      0.92      0.61     56961\n",
      "weighted avg       1.00      0.99      0.99     56961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "print('CONFUSION MATRIX\\n')\n",
    "print('Total number of')\n",
    "print('\\tTrue positives:\\t\\t', cm[1,1])\n",
    "print('\\tTrue negatives:\\t\\t', cm[0,0])\n",
    "print('\\tFalse positives:\\t', cm[0,1], '\\t\\tType I error')\n",
    "print('\\tFalse negatives:\\t', cm[1,0], '\\t\\tType II error')\n",
    "print('\\n')\n",
    "print('Correct classifications:\\t', np.round(100*(cm[0,0]+cm[1,1])/len(X_test),2), '%')\n",
    "print('Incorrect classifications:\\t', np.round(100*(cm[1,0]+cm[0,1])/len(X_test),2), '%')\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we can also look at the average precision-recall score and precision-recall curve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.12\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(y_test, predictions)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAFNCAYAAAAD7RaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGmZJREFUeJzt3XuUZWV95vHvY9OICl6wo0NopFHBQFARW8DlTMTBGCAjbdQRSIxiUIwR0Wh0yMpFRGdMNJqJEaNkUPEOmhltr8yIKMYBQhO8cJGxRYQGEkUE5SI0+Js/9i5yLE5Xne6uXdX11vez1ll99t7v2ed33q5VT+293/PuVBWSJKk991noAiRJ0jAMeUmSGmXIS5LUKENekqRGGfKSJDXKkJckqVGGvCRJjTLkteQkuSrJ7UluSfKvSd6XZMd+25eT/KzfdkOS/5lkl5HXnpRkY7996nHTyPYkOSHJJUluTbIhyceTPLbf/v4kbxppf2ySbyf5aV/LZ5PstIm2903y5iRX9/V/J8lrk2SkzVT9u42se3qSq2boj9HPdFOS/5vkySPbD07y82mf+ZZpbX4jybn95/hhkq8kOWLa+xycpJK8btr6Vf367TZR24c2Vfu0trP1/ZeTvHhMTRtGlqt/7S1Jrk3y9iTL+m3vSfKBMe/7uCR3JNl5tp8Pab4Z8lqqnllVOwL7A08C/nRk2/H9tkcDOwJ/Ne21Z1TVjiOPB49s+xvglcAJwM7AXsAngd+cXkCSpwL/DTi6qnYC9gbOnKHmjwOHAIcDOwG/CxzXv+eoW4E/m2E/45zRf+YVwDn9e426btpn3rGqzus/x3P79h8AVgIPB/4ceOa0fbwQuLH/dwgT9/0sHt/3xVOBI4Hf69e/H3h2kgdMa/8C4DNVdWO/PNPPhzSvDHktaVV1LfB5YN8x226iC4n9JtlXkj2Bl9OF9peq6o6quq2qPlxVfzHmJU8Czquqi/v3u7GqTq+qn47Z9yHAM4DnVNUlVXVXVZ0PPB94eZJHjzR/B3D0tHUTqaq7gA8Duyb5pQk+c4C3A2+sqv9RVTdX1c+r6itV9ZKRdvcHnkvXP3smWb25tc1Sx+b2/ayqaj3wNfr///6PmmuB54y87zLgt4HTt/YzSEMw5LWk9ae1DwcuHrPtocCzgfUT7u4QYENV/dOE7S8AfiPJG5I8Jcl9Z2j768AFVXXN6MqqugDY0L/3lGuBvwdOmrCOeyTZnu7I9EfAjyd4yWOA3YBPzNLuOcAtdEf8Z/XvMZc2t+9nleRXgP/AL/7/f4BfrP3pwHK6PxSlbY4hr6Xqk/210n8EvkJ32nzKO5LcDNxAd/r6FdNe+7z+2vXU45x+/UOB6yctoKq+SvdHxP7AZ4EfjV4DnmbFDPu+vt8+6s3AM5P86oTlPK/vj9uBlwDP7Y/qp/zytM98U3/a+qEjNczkhXSnse8GPkJ3pmH5hLVNYrP6fhb/nORW4HLgy8C7RrZ9EHhqkpX98guAj1TVxpE2m/r5kOadIa+l6llV9eCq2r2q/qCqbh/ZdkJVPQh4HPAQuuvMo87sXzv1eFq//kfALmyGqvp8VT2T7hryGuAY4MVjmt4ww7536beP7veHwDuBk0fXJ/mdkQFho0efZ/bXjh8OXAI8cdp7XDftMz+4qm6l+8xTNYzVny15Gt1lAIBPATuw+dfKZzJJ399Fd9Q9ajmwcdq6/enGYhwJHAjccw2+qq4GzgWen26w5rO496n6Tf18SPPOkJc2oaq+BbwJOGV0BPsMzgZWbsn15v469tnAlxgzPgD4InDg6Kh5gCQH0J0u/9KY17yVLlzvCez+GvXUgLDDxtRxA/BS4KSMfKtgBlcA1zBynXqM36X7XfPpJP8CXEkX8nN5yn6Svr8aWDVt3R7A96c3rM6ZwHl0gwhHnU5X+3OA71XVP29p0dLQDHlpZqcDDwOOmK1hVX2H7tTuR/uvZm2fZIckRyU5cXr7JGv6bQ/pv/51AN2I7vPH7PuLdEH2D0l+NcmyJAfRHR3/Xf/e019zE/A24HXTt83yOb5Nd9181tdVd6/qVwN/luRFSR6Y5D5J/n2SU/tmLwDeQDeAberxHOA3+3EPU+7b99fUY+r3032mrb/X2IUJ+/4M4EVJDuj7ey/gD4GPzfAR/wI4Lsm/G1n3D3R/WL0BB9xpG2fISzOoqjvpRquPfiXtyNz7O+MP67edQHea/BTgJuC7wG8Bnx6z+x/TXf/+DvAT4EPAW6vqw2PaQheM5wBfoBvE9iHgNO49ZmDU3wB3z/pB7+2tdOE29bl+ecxnfg5AVX2Cf/uq2XXAv9KdAflU/4fIKuCUqvqXkcdaugFtR4+85y10YwKmHv+xX3/0tPXf3UTNM/Z9VZ0FnAi8D7gZ+BxdSJ86bmf9a75FN2bjtSPrbuXfgn7c/9VMPx/SvEr3h7gkSWqNR/KSJDXKkJckqVGGvCRJjTLkJUlqlCEvSVKj7nVrx23dihUratWqVQtdhiRJ8+Kiiy66oapmvWHUOIsu5FetWsW6desWugxJkuZFknvNyjgpT9dLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMGC/kk703ygySXbGJ7krwjyfok30yy/1C1SJK0FA15JP9+4NAZth8G7Nk/jgP+bsBaJElacgYL+ao6F7hxhiZrgA9U53zgwUl2GaoeSZKWmoW8Jr8rcM3I8oZ+3Yyuu/n2wQqSJKklCxnyGbOuxjZMjkuyLsm6n956x8BlSZLUhoUM+Q3AbiPLK4HrxjWsqlOranVVrV6+fPm8FCdJ0mK3kCG/FnhBP8r+IODmqrp+AeuRJKkpg91qNslHgYOBFUk2AK8HlgNU1buBzwGHA+uB24AXDVWLJElL0WAhX1VHz7K9gJcP9f6SJC11zngnSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGmXIS5LUKENekqRGGfKSJDXKkJckqVGGvCRJjTLkJUlqlCEvSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGmXIS5LUqEFDPsmhSa5Isj7JiWO2PyLJOUkuTvLNJIcPWY8kSUvJYCGfZBlwCnAYsA9wdJJ9pjX7U+DMqnoCcBTwrqHqkSRpqRnySP4AYH1VXVlVdwIfA9ZMa1PAA/vnDwKuG7AeSZKWlO0G3PeuwDUjyxuAA6e1OQn430leATwAePqA9UiStKQMeSSfMetq2vLRwPuraiVwOPDBJPeqKclxSdYlWbdx48YBSpUkqT1DhvwGYLeR5ZXc+3T8scCZAFV1HrADsGL6jqrq1KpaXVWrly9fPlC5kiS1ZciQvxDYM8keSbanG1i3dlqbq4FDAJLsTRfyPxywJkmSlozBQr6q7gKOB84CLqcbRX9pkpOTHNE3ew3wkiTfAD4KHFNV00/pS5KkLZDFlqk777533fj9yxe6DEmS5kWSi6pq9Za81hnvJElqlCEvSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGmXIS5LUKENekqRGGfKSJDXKkJckqVGGvCRJjTLkJUlqlCEvSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGjVoyCc5NMkVSdYnOXETbZ6X5LIklyb5yJD1SJK0lGw31I6TLANOAX4d2ABcmGRtVV020mZP4I+Bp1TVj5M8bKh6JElaaoY8kj8AWF9VV1bVncDHgDXT2rwEOKWqfgxQVT8YsB5JkpaUiY/kk+wK7D76mqo6d4aX7ApcM7K8AThwWpu9+n1/DVgGnFRVXxjz3scBxwHsuMujJi1ZkqQlbaKQT/KXwJHAZcDd/eoCZgr5jFlXY95/T+BgYCXw1ST7VtVNv/CiqlOBUwF23n3v6fuQJEljTHok/yzgMVV1x2bsewOw28jySuC6MW3Or6qNwPeSXEEX+hduxvtIkqQxJr0mfyWwfDP3fSGwZ5I9kmwPHAWsndbmk8DTAJKsoDt9f+Vmvo8kSRpj0iP524CvJzkbuOdovqpO2NQLququJMcDZ9Fdb39vVV2a5GRgXVWt7bc9I8nUZYDXVtWPtvCzSJKkEama/RJ3kheOW19Vp895RbPYefe968bvXz7fbytJ0oJIclFVrd6S1050JF9Vp/en3PfqV13RX0eXJEnbqElH1x8MnA5cRTdqfrckL5zlK3SSJGkBTXpN/m3AM6rqCoAkewEfBZ44VGGSJGnrTDq6fvlUwANU1f9j80fbS5KkeTTpkfy6JKcBH+yXfwe4aJiSJEnSXJg05F8GvBw4ge6a/LnAu4YqSpIkbb1JR9ffAby9f0iSpEVgxpBPcmZVPS/Jt7j3vPNU1eMGq0ySJG2V2Y7kX9n/+5+GLkSSJM2tGUfXV9X1/dMbgGuq6vvAfYHHc++bzUiSpG3IpF+hOxfYob+n/NnAi4D3D1WUJEnaepOGfKrqNuDZwN9W1W8B+wxXliRJ2loTh3ySJ9N9P/6z/bpJv34nSZIWwKQh/yrgj4H/1d8u9pHAOcOVJUmSttak35P/CvCVkeUr6SbGkSRJ26jZvif/36vqVUk+zfjvyR8xWGWSJGmrzHYkPzVX/V8NXYgkSZpbM4Z8VU3dhGYdcHtV/RwgyTK678tLkqRt1KQD784G7j+yfD/gi3NfjiRJmiuThvwOVXXL1EL//P4ztJckSQts0pC/Ncn+UwtJngjcPkxJkiRpLkw6oc2rgI8nmZqvfhfgyGFKkiRJc2HS78lfmORXgMcAAb5dVRsHrUySJG2ViU7XJ7k/8F+AV1bVt4BVSbz9rCRJ27BJr8m/D7gTeHK/vAF40yAVSZKkOTFpyD+qqt4CbASoqtvpTttLkqRt1KQhf2eS+9FPbZvkUcAdg1UlSZK22qSj618PfAHYLcmHgacAxwxVlCRJ2nqzhnySAN8Gng0cRHea/pVVdcPAtUmSpK0wa8hXVSX5ZFU9EfjsPNQkSZLmwKTX5M9P8qRBK5EkSXNq0mvyTwN+P8lVwK10p+yrqh43VGGSJGnrTBryhw1ahSRJmnMzhnySHYDfBx4NfAs4rarumo/CJEnS1pntmvzpwGq6gD8MeNvgFUmSpDkx2+n6farqsQBJTgP+afiSJEnSXJjtSP6eO815ml6SpMVltiP5xyf5Sf88wP365anR9Q8ctDpJkrTFZgz5qlo2X4VIkqS5NelkOJIkaZEx5CVJapQhL0lSowYN+SSHJrkiyfokJ87Q7rlJKsnqIeuRJGkpGSzkkywDTqGbRGcf4Ogk+4xptxNwAnDBULVIkrQUDXkkfwCwvqqurKo7gY8Ba8a0eyPwFuBnA9YiSdKSM2TI7wpcM7K8oV93jyRPAHarqs8MWIckSUvSkCGfMevqno3JfYC/Bl4z646S45KsS7Ju48aNszWXJEkMG/IbgN1GllcC140s7wTsC3y5v0/9QcDacYPvqurUqlpdVauXL18+YMmSJLVjyJC/ENgzyR5JtgeOAtZObayqm6tqRVWtqqpVwPnAEVW1bsCaJElaMgYL+f6GNscDZwGXA2dW1aVJTk5yxFDvK0mSOqmq2VttQ3befe+68fuXL3QZkiTNiyQXVdUWzSPjjHeSJDXKkJckqVGGvCRJjTLkJUlqlCEvSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGmXIS5LUKENekqRGGfKSJDXKkJckqVGGvCRJjTLkJUlqlCEvSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNGjTkkxya5Iok65OcOGb7q5NcluSbSc5OsvuQ9UiStJQMFvJJlgGnAIcB+wBHJ9lnWrOLgdVV9TjgE8BbhqpHkqSlZsgj+QOA9VV1ZVXdCXwMWDPaoKrOqarb+sXzgZUD1iNJ0pIyZMjvClwzsryhX7cpxwKfH7AeSZKWlO0G3HfGrKuxDZPnA6uBp25i+3HAcQA77vKouapPkqSmDXkkvwHYbWR5JXDd9EZJng78CXBEVd0xbkdVdWpVra6q1cuXLx+kWEmSWjNkyF8I7JlkjyTbA0cBa0cbJHkC8B66gP/BgLVIkrTkDBbyVXUXcDxwFnA5cGZVXZrk5CRH9M3eCuwIfDzJ15Os3cTuJEnSZkrV2Mvk26ydd9+7bvz+5QtdhiRJ8yLJRVW1ekte64x3kiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNGvJ+8oO4fePdHPme8xa6DEnSPFqz36789oGPWOgyFp1FdyR/v+XLFroESdI8uuz6n/Cpr1+70GUsSovuSP6Rv/QAznjpkxe6DEnSPPHs7ZZbdEfykiRpMoa8JEmNMuQlSWqUIS9JUqMMeUmSGmXIS5LUKENekqRGGfKSJDXKkJckqVGGvCRJjTLkJUlqlCEvSVKjDHlJkhplyEuS1ChDXpKkRhnykiQ1ypCXJKlRhrwkSY0y5CVJapQhL0lSowx5SZIaZchLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGjVoyCc5NMkVSdYnOXHM9vsmOaPffkGSVUPWI0nSUjJYyCdZBpwCHAbsAxydZJ9pzY4FflxVjwb+GvjLoeqRJGmp2W7AfR8ArK+qKwGSfAxYA1w20mYNcFL//BPAO5OkqmrAuiRJi8xl1/+EI99z3kKXsegMGfK7AteMLG8ADtxUm6q6K8nNwEOBGwasS5K0iKzZb9eFLmHRGjLkM2bd9CP0SdqQ5DjguH7xjiSXbGVtmtkK/ENrPtjPw7OPh2cfD+8xW/rCIUN+A7DbyPJK4LpNtNmQZDvgQcCN03dUVacCpwIkWVdVqwepWIB9PF/s5+HZx8Ozj4eXZN2WvnbI0fUXAnsm2SPJ9sBRwNppbdYCL+yfPxf4ktfjJUmaG4MdyffX2I8HzgKWAe+tqkuTnAysq6q1wGnAB5OspzuCP2qoeiRJWmqGPF1PVX0O+Ny0dX8+8vxnwH/ezN2eOgelaWb28fywn4dnHw/PPh7eFvdxPDsuSVKbnNZWkqRGbbMh75S4w5ugj1+d5LIk30xydpLdF6LOxWy2Ph5p99wklcRRyltgkn5O8rz+5/nSJB+Z7xoXuwl+XzwiyTlJLu5/Zxy+EHUuZknem+QHm/qaeDrv6P8Pvplk/1l3WlXb3INuoN53gUcC2wPfAPaZ1uYPgHf3z48CzljouhfTY8I+fhpw//75y+zjue/jvt1OwLnA+cDqha57sT0m/FneE7gYeEi//LCFrnsxPSbs41OBl/XP9wGuWui6F9sD+DVgf+CSTWw/HPg83RwzBwEXzLbPbfVI/p4pcavqTmBqStxRa4DT++efAA5JMm5yHY03ax9X1TlVdVu/eD7dXAea3CQ/xwBvBN4C/Gw+i2vIJP38EuCUqvoxQFX9YJ5rXOwm6eMCHtg/fxD3nhdFs6iqcxkzV8yINcAHqnM+8OAku8y0z2015MdNiTt9XsNfmBIXmJoSV5OZpI9HHUv3F6QmN2sfJ3kCsFtVfWY+C2vMJD/LewF7JflakvOTHDpv1bVhkj4+CXh+kg1036p6xfyUtqRs7u/tYb9CtxXmbEpcbdLE/Zfk+cBq4KmDVtSeGfs4yX3o7r54zHwV1KhJfpa3oztlfzDdGamvJtm3qm4auLZWTNLHRwPvr6q3JXky3Rwo+1bVz4cvb8nY7NzbVo/kN2dKXGaaElebNEkfk+TpwJ8AR1TVHfNUWytm6+OdgH2BLye5iu4a21oH3222SX9ffKqqNlbV94Ar6EJfk5mkj48FzgSoqvOAHejmtdfcmej39qhtNeSdEnd4s/Zxfyr5PXQB7zXMzTdjH1fVzVW1oqpWVdUqunEPR1TVFs9TvURN8vvik3QDSUmygu70/ZXzWuXiNkkfXw0cApBkb7qQ/+G8Vtm+tcAL+lH2BwE3V9X1M71gmzxdX06JO7gJ+/itwI7Ax/sxjVdX1RELVvQiM2EfaytN2M9nAc9IchlwN/DaqvrRwlW9uEzYx68B/j7JH9KdQj7GA6/Nk+SjdJeUVvRjG14PLAeoqnfTjXU4HFgP3Aa8aNZ9+n8gSVKbttXT9ZIkaSsZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS8tMUnuTvL1JJck+XSSB8/x/o9J8s7++UlJ/mgu9y9pcoa8tPTcXlX7VdW+dHNMvHyhC5I0DENeWtrOY+QGF0lem+TC/l7VbxhZ/4J+3TeSfLBf98wkF/T3D/9ikocvQP2SZrBNzngnaXhJltFNQ3pav/wMuvncD6C7EcbaJL8G/Iju/gVPqaobkuzc7+IfgYOqqpK8GHgd3axnkrYRhry09NwvydeBVcBFwP/p1z+jf1zcL+9IF/qPBz5RVTcAVNXUjaBWAmf097PeHvjevFQvaWKerpeWnturaj9gd7pwnromH+DN/fX6/arq0VV1Wr9+3PzXfwu8s6oeC7yU7oYkkrYhhry0RFXVzcAJwB8lWU5385HfS7IjQJJdkzwMOBt4XpKH9uunTtc/CLi2f/5CJG1zPF0vLWFVdXGSbwBHVdUH+1uEntffdfAW4Pn93cb+K/CVJHfTnc4/BjiJ7g6F19LdJnePhfgMkjbNu9BJktQoT9dLktQoQ16SpEYZ8pIkNcqQlySpUYa8JEmNMuQlSWqUIS9JUqMMeUmSGvX/AXRe7Hy0h/QsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision = dict()\n",
    "recall = dict()\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.step(recall, precision, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('PRECISION-RECALL CURVE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Conclusion\n",
    "\n",
    "The Logistic Regression model performed very well in terms of recall score for both fraudulent and non-fraudulent transactions. Also, most incorrect predictions turned out to be False Positives, which remain much better than False Negatives. Indeed, it is largely preferable to predict a False Positive, which would then allow an agent to perform a deeper investigation and find out whether that transaction was actually fraudulent or not. \n",
    "\n",
    "The overall precision of the model could still be improved, either by decreasing the imbalance of the dataset (i.e. by including more fraudulent transactions), or by testing other classification approaches (e.g. Support Vector Machine, Artificial Neural Network).\n",
    "\n",
    "**This pipeline could easily be transferred to other logistic regression problems.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
