{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-2 Local Inference in Jupyter Notebook\n",
    "\n",
    "This notebook enables you to harness the power of the Llama-2 language model, boasting an impressive 7 billion parameters, for text generation tasks. With this tool, you can define custom prompts and run Llama-2 locally on your own GPU, ensuring both privacy and control over your data.\n",
    "\n",
    "## Features:\n",
    "\n",
    "- **Local Execution**: Run Llama-2 on your personal GPU, avoiding the need for external servers or cloud services.\n",
    "\n",
    "- **Custom Prompts**: Craft specific prompts to generate tailored responses from the model.\n",
    "\n",
    "- **High Performance**: Leverage the full capacity of the 7B parameter Llama-2 model for natural language understanding and generation.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Before using this tool, you need to set up the environment and access the Llama-2 model. Follow these steps for installation:\n",
    "\n",
    "### Installation\n",
    "\n",
    "To set up the environment for this project, follow these steps:\n",
    "\n",
    "1. **Create a Project Directory**\n",
    "\n",
    "   - Create a dedicated project directory where you want to organize all the project-related files and the Llama repository. You can do this in your terminal (Linux or VS Code), for example:\n",
    "     ```\n",
    "     mkdir llama-project\n",
    "     cd llama-project\n",
    "     ```\n",
    "\n",
    "2. **Clone Meta AI Llama Repository**\n",
    "\n",
    "   - Clone the Llama repository into your project directory:\n",
    "     ```\n",
    "     git clone https://github.com/facebookresearch/llama.git\n",
    "     ```\n",
    "\n",
    "   - Make sure that your current Jupyter notebook or script is located within this project directory for easier access to project files.\n",
    "\n",
    "3. **Request Permission to Access Llama-2**\n",
    "\n",
    "   - Go to [Meta AI Llama Downloads](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
    "   - Fill in your information and select the `Llama 2 & Llama Chat` checkbox.\n",
    "   - Accept the terms and conditions, then click `Accept and Continue`.\n",
    "   - You will receive an email from Meta with a custom URL for accessing Llama 2.\n",
    "\n",
    "4. **Download Models**\n",
    "\n",
    "   - Change directory to the cloned `llama` repository within your project directory:\n",
    "     ```\n",
    "     cd llama\n",
    "     ```\n",
    "   - Run the following commands (Linux):\n",
    "     ```\n",
    "     chmod 755 download.sh\n",
    "     ./download.sh\n",
    "     ```\n",
    "\n",
    "5. **Create a Virtual Environment**\n",
    "\n",
    "   - If you are still in the `llama` directory, return to the project directory by going back one level:\n",
    "     ```\n",
    "     cd ../\n",
    "     ```\n",
    "   - Create a virtual environment using the following command (using Python 3.11 as an example):\n",
    "     ```\n",
    "     python3.11 -m venv llama-venv\n",
    "     ```\n",
    "\n",
    "6. **Install Requirements**\n",
    "\n",
    "   - Activate your virtual environment:\n",
    "     ```\n",
    "     source llama-venv/bin/activate\n",
    "     ```\n",
    "   - Change directory back to the `llama` repository within your project directory:\n",
    "     ```\n",
    "     cd llama\n",
    "     ```\n",
    "   - Run the following command to install all the required Python packages in your virtual environment:\n",
    "     ```\n",
    "     pip install -e .\n",
    "     ```\n",
    "\n",
    "7. **Download 'complete_text.py'**\n",
    "\n",
    "   - Download the 'complete_text.py' file from [your source] (replace with your source URL).\n",
    "   - Move the downloaded 'complete_text.py' file to the 'llama' repository directory within your project.\n",
    "\n",
    "Now that you have successfully set up your environment and organized your project directory, you can proceed to define model parameters, prompts, and run the Llama-2 model for text generation tasks.\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. **Initialization**\n",
    "\n",
    "   Before using the Llama-2 model, ensure you run the Initialization cell to import all the required libraries and define essential helper functions.\n",
    "\n",
    "2. **Define Model Parameters**\n",
    "\n",
    "   You can customize the behavior of the Llama-2 model by specifying various parameters:\n",
    "\n",
    "   - **Model Name**: The name of the model used.\n",
    "     ```python\n",
    "     model_name = \"llama-2-7b-chat\"\n",
    "     ```\n",
    "\n",
    "   - **Llama Directory**: The path to the directory containing the cloned Llama repository.\n",
    "     ```python\n",
    "     llama_dir = \"../llama\"\n",
    "     ```\n",
    "\n",
    "   - **Complete Text Path**: The path to the complete text function (used to generate responses).\n",
    "     ```python\n",
    "     complete_text_path = f\"{llama_dir}/complete_text.py\"\n",
    "     ```\n",
    "\n",
    "   - **Checkpoint Directory**: The directory containing checkpoint files for the pretrained model.\n",
    "     ```python\n",
    "     ckpt_dir = f\"{llama_dir}/{model_name}\"\n",
    "     ```\n",
    "\n",
    "   - **Tokenizer Path**: The path to the tokenizer model used for text encoding and decoding.\n",
    "     ```python\n",
    "     tokenizer_path = f\"{llama_dir}/tokenizer.model\"\n",
    "     ```\n",
    "\n",
    "   - **Temperature**: A parameter controlling randomness in generation. Lower values make output less random.\n",
    "     ```python\n",
    "     temperature = 0.2\n",
    "     ```\n",
    "\n",
    "   - **Top-p Sampling**: A parameter controlling diversity in generation. Higher values result in more diverse output.\n",
    "     ```python\n",
    "     top_p = 0.9\n",
    "     ```\n",
    "\n",
    "   - **Maximum Sequence Length**: The maximum length of input prompts (in number of characters).\n",
    "     ```python\n",
    "     max_seq_len = 4096\n",
    "     ```\n",
    "\n",
    "   - **Maximum Generated Length**: The maximum length of generated sequences.\n",
    "     ```python\n",
    "     max_gen_len = 4096\n",
    "     ```\n",
    "\n",
    "   - **Maximum Batch Size**: The maximum batch size for generating sequences.\n",
    "     ```python\n",
    "     max_batch_size = 4\n",
    "     ```\n",
    "\n",
    "3. **Define Your Prompt**\n",
    "\n",
    "   Define your specific prompt in the code, tailoring it to your text generation task.\n",
    "\n",
    "4. **Run the Model**\n",
    "\n",
    "   Execute the script to run the Llama-2 model with your defined prompt. The model will generate a response based on your input, leveraging the specified parameters.\n",
    "\n",
    "\n",
    "## Troubleshooting Tips\n",
    "\n",
    "If you encounter issues while setting up or running the Llama-2 model, consider these troubleshooting tips:\n",
    "\n",
    "### 1. GPU and Nvidia Drivers\n",
    "\n",
    "**Problem:** The code relies on a GPU with the right Nvidia drivers installed, including the CUDA Toolkit.\n",
    "\n",
    "**Solution:** Ensure that your computer has a compatible GPU with up-to-date Nvidia drivers and the CUDA Toolkit installed. Verify that your GPU is recognized and accessible by the code.\n",
    "\n",
    "### 2. GPU Memory Limitation\n",
    "\n",
    "**Problem:** Your local GPU may not have enough memory to run the model with certain parameter settings.\n",
    "\n",
    "**Solution:** If you encounter memory-related errors, consider reducing the `max_seq_len` and `max_gen_len` parameters to a more manageable size, like 512 and 256, as they have been known to work successfully on some configurations. Experiment with smaller values to find a balance between performance and available GPU memory.\n",
    "\n",
    "### 3. Parameter Configuration\n",
    "\n",
    "**Problem:** Incorrectly set parameters, including file and directory paths, can lead to errors.\n",
    "\n",
    "**Solution:** Double-check all parameter settings, including file paths and directory locations. Ensure that they point to the correct files and directories required for the code to run successfully.\n",
    "\n",
    "If you continue to experience issues, consult the official Llama-2 documentation or seek assistance from the support channels provided by the model's creators for more specific troubleshooting and guidance.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Imports\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "import subprocess\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Helper function\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "def format_and_print_string(input_string, line_length=79):\n",
    "    \"\"\"\n",
    "    Format and print a string with escaped characters to make it readable.\n",
    "\n",
    "    This function takes an input string containing escaped characters such as '\\\\n' and '\\\\\"'\n",
    "    and formats it to replace those escape sequences with their actual representations.\n",
    "    It then prints the formatted string with proper line breaks.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The input string with escaped characters to be formatted and printed.\n",
    "        line_length (int): The maximum line length before wrapping. Defaults to 79.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value; it prints the formatted and wrapped string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace escaped characters with their actual representations\n",
    "    formatted_string = input_string.replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
    "\n",
    "    # Wrap lines longer than line_length while preserving leading whitespace\n",
    "    wrapped_lines = []\n",
    "    for line in formatted_string.splitlines():\n",
    "        wrapped_lines.extend(textwrap.wrap(line, width=line_length, subsequent_indent=''))\n",
    "\n",
    "    # Join the wrapped lines and print the result\n",
    "    wrapped_string = '\\n'.join(wrapped_lines)\n",
    "    print(wrapped_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Model parameters\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "# The name of the model used\n",
    "model_name = \"llama-2-7b-chat\"\n",
    "# The path to the directory containing the llama cloned repository\n",
    "llama_dir = \"../llama\"\n",
    "# The path to the complete text function (used to generate response)\n",
    "complete_text_path = f\"{llama_dir}/complete_text.py\"\n",
    "# The directory containing checkpoint files for the pretrained model\n",
    "ckpt_dir = f\"{llama_dir}/{model_name}\"\n",
    "# The path to the tokenizer model used for text encoding/decoding\n",
    "tokenizer_path= f\"{llama_dir}/tokenizer.model\"\n",
    "# The temperature value for controlling randomness in generation\n",
    "# (the lower the less random)\n",
    "temperature = 0.2\n",
    "# The top-p sampling parameter for controlling diversity in generation\n",
    "top_p = 0.9\n",
    "# The maximum sequence length for input prompts (in number of characters)\n",
    "max_seq_len = 512\n",
    "# The maximum length of generated sequences.\n",
    "max_gen_len = 256\n",
    "# The maximum batch size for generating sequences\n",
    "max_batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Prompt\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "# Define prompt used to generate response\n",
    "prompt = 'Say Hi in 5 different languages.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate response\n",
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Run model and generate response\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "# Define the command to run the script with specified arguments\n",
    "command = [\n",
    "    \"torchrun\",  # Replace with the actual command you need\n",
    "    \"--nproc_per_node\",\n",
    "    \"1\",\n",
    "    f\"{complete_text_path}\",\n",
    "    \"--ckpt_dir\",\n",
    "    f\"{ckpt_dir}\",\n",
    "    \"--tokenizer_path\",\n",
    "    f\"{tokenizer_path}\",\n",
    "    \"--max_seq_len\",\n",
    "    f\"{max_seq_len}\",\n",
    "    \"--max_gen_len\",\n",
    "    f\"{max_gen_len}\",\n",
    "    \"--max_batch_size\",\n",
    "    f\"{max_batch_size}\",\n",
    "    \"--prompt\",\n",
    "    f\"{prompt}\"\n",
    "]\n",
    "\n",
    "# Execute the command and capture the output\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "stdout, stderr = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --------------------------------------------------------------------------- #\n",
      "# ORIGINAL PROMPT\n",
      "# --------------------------------------------------------------------------- #\n",
      "\n",
      "\n",
      "Say Hi in 5 different languages.\n",
      "\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------------- #\n",
      "# RESPONSE\n",
      "# --------------------------------------------------------------------------- #\n",
      "\n",
      "\n",
      "Saying \"Hi\" in different languages can be a fun and interesting way to connect\n",
      "with people from different cultures. Here are five ways to say \"Hi\" in\n",
      "different languages:\n",
      "1. Spanish: Hola (OH-lah)\n",
      "2. French: Bonjour (bone-JOOR)\n",
      "3. German: Hallo (HA-lo)\n",
      "4. Italian: Ciao (CHOW)\n",
      "5. Chinese: 你好 (nǐ hǎo) (Mandarin) or 您好 (nín hǎo) (Cantonese)\n",
      "Note: Pronunciation is approximate and may vary depending on the speaker and\n",
      "dialect.\"\n"
     ]
    }
   ],
   "source": [
    "# Check for error and display model output\n",
    "if process.returncode != 0:\n",
    "    print(f\"Error: {stderr}\")\n",
    "else:\n",
    "    print(\"# --------------------------------------------------------------------------- #\")\n",
    "    print(f\"# ORIGINAL PROMPT\")\n",
    "    print(\"# --------------------------------------------------------------------------- #\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"{prompt}\")\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"# --------------------------------------------------------------------------- #\")\n",
    "    print(f\"# RESPONSE\")\n",
    "    print(\"# --------------------------------------------------------------------------- #\")\n",
    "    print(\"\\n\")\n",
    "    # Select response from STDOUT\n",
    "    escaped_string = stdout.split('[')[1].split(f\"{prompt}\")[1][4:-2]\n",
    "    # Format and print model response\n",
    "    format_and_print_string(escaped_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
